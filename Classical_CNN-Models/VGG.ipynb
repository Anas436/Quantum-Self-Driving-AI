{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWokrBVoAsy",
        "outputId": "1a09c1d1-8521-44dd-93b9-010a6303ac20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Omdena-Quantum-Self-Driving'...\n",
            "remote: Enumerating objects: 45753, done.\u001b[K\n",
            "remote: Counting objects: 100% (11663/11663), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11648/11648), done.\u001b[K\n",
            "remote: Total 45753 (delta 26), reused 11639 (delta 14), pack-reused 34090\u001b[K\n",
            "Receiving objects: 100% (45753/45753), 2.16 GiB | 43.03 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Updating files: 100% (45592/45592), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/raghav-thiruv/Omdena-Quantum-Self-Driving.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"/content/Omdena-Quantum-Self-Driving/Images/driving_dataset1/data.txt\" \"/content/Omdena-Quantum-Self-Driving/Images/\""
      ],
      "metadata": {
        "id": "Tmt5PqsGyloF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !du -h /content/Omdena-Quantum-Self-Driving"
      ],
      "metadata": {
        "id": "FIwtn2nbtzUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from struct import unpack\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "img_dir = '/content/Omdena-Quantum-Self-Driving/Images/driving_dataset1/'\n",
        "data_file = \"/content/Omdena-Quantum-Self-Driving/Images/data.txt\""
      ],
      "metadata": {
        "id": "dtxK3p_RpaRb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "nVE09aTTrMep"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    flatten_output = Flatten()(base_model.output)\n",
        "\n",
        "    angle_input = Input(shape=(1,))\n",
        "    fused_output = Concatenate()([flatten_output, angle_input])\n",
        "\n",
        "    fc1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(fused_output)\n",
        "    dropout1 = Dropout(0.2)(fc1)\n",
        "    fc2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(fc1)\n",
        "    output = Dense(1)(fc2)\n",
        "\n",
        "    model = Model(inputs=[base_model.input, angle_input], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Load and preprocess a single image\n",
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def create_dataset(image_files, angles):\n",
        "    image_paths = [os.path.join(img_dir, file_name) for file_name in image_files]\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    angle_dataset = tf.data.Dataset.from_tensor_slices(angles)\n",
        "    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = tf.data.Dataset.zip((image_dataset, angle_dataset))\n",
        "    return dataset\n",
        "\n",
        "# Get the list of image files\n",
        "image_files = []\n",
        "\n",
        "# Load the steering angles\n",
        "angles_dict = {}\n",
        "with open(data_file, 'r') as file:\n",
        "    for line in file:\n",
        "        img_file, angle = line.strip().split()\n",
        "        image_files.append(img_file)\n",
        "        angles_dict[img_file] = float(angle)\n",
        "\n",
        "# Get the angles for the image files\n",
        "angles = [angles_dict[file_name] for file_name in image_files]\n",
        "\n",
        "# Split the image files into training and validation sets\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(image_files))\n",
        "train_image_files = image_files[num_validation_samples:]\n",
        "train_image_angles = angles[num_validation_samples:]\n",
        "val_image_files = image_files[:num_validation_samples]\n",
        "val_image_angles = angles[:num_validation_samples]\n",
        "\n",
        "# Create the training and validation datasets with both inputs\n",
        "train_dataset = create_dataset(train_image_files, train_image_angles)\n",
        "val_dataset = create_dataset(val_image_files, val_image_angles)\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Define the input shape for the angle dataset\n",
        "angle_input_shape = (1,)\n",
        "\n",
        "# Create the VGG-16 model\n",
        "model = build_model()\n",
        "\n",
        "# Define the mean squared error as the loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=mse_loss)\n",
        "\n",
        "# Modify the fit call to pass both inputs\n",
        "train_dataset = train_dataset.map(lambda img, angle: ((img, angle), angle))\n",
        "val_dataset = val_dataset.map(lambda img, angle: ((img, angle), angle))"
      ],
      "metadata": {
        "id": "hwcojnuXo4Fx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 50\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMoym7S0qb-T",
        "outputId": "217b96f0-f8e4-4a1b-9552-ba98f5a47465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1136/1136 [==============================] - 533s 463ms/step - loss: 111.2662 - val_loss: 2459.2766\n",
            "Epoch 2/50\n",
            "1136/1136 [==============================] - 525s 459ms/step - loss: 47.2643 - val_loss: 2370.3228\n",
            "Epoch 3/50\n",
            "1136/1136 [==============================] - 523s 457ms/step - loss: 33.1392 - val_loss: 2320.9272\n",
            "Epoch 4/50\n",
            "1136/1136 [==============================] - 521s 456ms/step - loss: 30.0409 - val_loss: 2167.6414\n",
            "Epoch 5/50\n",
            "1136/1136 [==============================] - 521s 455ms/step - loss: 25.8302 - val_loss: 1935.6454\n",
            "Epoch 6/50\n",
            "1136/1136 [==============================] - 524s 457ms/step - loss: 24.7835 - val_loss: 1719.2152\n",
            "Epoch 7/50\n",
            "1136/1136 [==============================] - 522s 457ms/step - loss: 20.7051 - val_loss: 1603.2896\n",
            "Epoch 8/50\n",
            "1136/1136 [==============================] - 521s 456ms/step - loss: 18.3696 - val_loss: 1328.1682\n",
            "Epoch 9/50\n",
            "1136/1136 [==============================] - 523s 457ms/step - loss: 16.0950 - val_loss: 1159.1401\n",
            "Epoch 10/50\n",
            "1136/1136 [==============================] - 519s 454ms/step - loss: 13.5496 - val_loss: 1027.3414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "marker_mapping = {\n",
        "    0xffd8: \"Start of Image\",\n",
        "    0xffe0: \"Application Default Header\",\n",
        "    0xffdb: \"Quantization Table\",\n",
        "    0xffc0: \"Start of Frame\",\n",
        "    0xffc4: \"Define Huffman Table\",\n",
        "    0xffda: \"Start of Scan\",\n",
        "    0xffd9: \"End of Image\"\n",
        "}\n",
        "\n",
        "\n",
        "class JPEG:\n",
        "    def __init__(self, image_file):\n",
        "        with open(image_file, 'rb') as f:\n",
        "            self.img_data = f.read()\n",
        "\n",
        "    def decode(self):\n",
        "        data = self.img_data\n",
        "        while(True):\n",
        "            marker, = unpack(\">H\", data[0:2])\n",
        "            # print(marker_mapping.get(marker))\n",
        "            if marker == 0xffd8:\n",
        "                data = data[2:]\n",
        "            elif marker == 0xffd9:\n",
        "                return\n",
        "            elif marker == 0xffda:\n",
        "                data = data[-2:]\n",
        "            else:\n",
        "                lenchunk, = unpack(\">H\", data[2:4])\n",
        "                data = data[2+lenchunk:]\n",
        "            if len(data)==0:\n",
        "               raise TypeError(\"issue reading jpeg file\")\n",
        "\n",
        "\n",
        "bads = []\n",
        "\n",
        "for dirName, subdirList, fileList in os.walk(img_dir):\n",
        "    imagesList = fileList\n",
        "    for img in tqdm(imagesList):\n",
        "      image = os.path.join(img_dir,img)\n",
        "      image = JPEG(image)\n",
        "      try:\n",
        "        image.decode()\n",
        "      except:\n",
        "        bads.append(img)\n",
        "\n",
        "\n",
        "for name in bads:\n",
        "  os.remove(os.path.join(img_dir,name))"
      ],
      "metadata": {
        "id": "dx_QL2romLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386e059a-c37a-472a-8ffa-ad2329bed1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45568/45568 [00:03<00:00, 13119.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPJVoAUlrSih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}