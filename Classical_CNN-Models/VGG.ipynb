{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWokrBVoAsy",
        "outputId": "e3a02f7f-0936-49e4-d316-9bf7c3a8028c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Omdena-Quantum-Self-Driving'...\n",
            "remote: Enumerating objects: 45761, done.\u001b[K\n",
            "remote: Counting objects: 100% (11671/11671), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11653/11653), done.\u001b[K\n",
            "remote: Total 45761 (delta 32), reused 11645 (delta 17), pack-reused 34090\n",
            "Receiving objects: 100% (45761/45761), 2.16 GiB | 39.25 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "Updating files: 100% (45593/45593), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/raghav-thiruv/Omdena-Quantum-Self-Driving.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"/content/Omdena-Quantum-Self-Driving/Images/driving_dataset1/data.txt\" \"/content/Omdena-Quantum-Self-Driving/Images/\""
      ],
      "metadata": {
        "id": "Tmt5PqsGyloF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !du -h /content/Omdena-Quantum-Self-Driving"
      ],
      "metadata": {
        "id": "FIwtn2nbtzUn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from struct import unpack\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "img_dir = '/content/Omdena-Quantum-Self-Driving/Images/driving_dataset1/'\n",
        "data_file = \"/content/Omdena-Quantum-Self-Driving/Images/data.txt\""
      ],
      "metadata": {
        "id": "dtxK3p_RpaRb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "nVE09aTTrMep"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    flatten_output = Flatten()(base_model.output)\n",
        "\n",
        "    angle_input = Input(shape=(1,))\n",
        "    fused_output = Concatenate()([flatten_output, angle_input])\n",
        "\n",
        "    fc1 = Dense(512, activation='relu')(fused_output)\n",
        "    # dropout1 = Dropout(0.3)(fc1)\n",
        "    fc2 = Dense(256, activation='relu')(fc1)\n",
        "    output = Dense(1)(fc2)\n",
        "\n",
        "    model = Model(inputs=[base_model.input, angle_input], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Load and preprocess a single image\n",
        "def load_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def create_dataset(image_files, angles):\n",
        "    image_paths = [os.path.join(img_dir, file_name) for file_name in image_files]\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    angle_dataset = tf.data.Dataset.from_tensor_slices(angles)\n",
        "    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = tf.data.Dataset.zip((image_dataset, angle_dataset))\n",
        "    return dataset\n",
        "\n",
        "# Get the list of image files\n",
        "image_files = []\n",
        "\n",
        "# Load the steering angles\n",
        "angles_dict = {}\n",
        "with open(data_file, 'r') as file:\n",
        "    for line in file:\n",
        "        img_file, angle = line.strip().split()\n",
        "        image_files.append(img_file)\n",
        "        angles_dict[img_file] = float(angle)\n",
        "\n",
        "# Get the angles for the image files\n",
        "angles = [angles_dict[file_name] for file_name in image_files]\n",
        "\n",
        "# Split the image files into training and validation sets\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(image_files))\n",
        "train_image_files = image_files[num_validation_samples:]\n",
        "train_image_angles = angles[num_validation_samples:]\n",
        "val_image_files = image_files[:num_validation_samples]\n",
        "val_image_angles = angles[:num_validation_samples]\n",
        "\n",
        "# Create the training and validation datasets with both inputs\n",
        "train_dataset = create_dataset(train_image_files, train_image_angles)\n",
        "val_dataset = create_dataset(val_image_files, val_image_angles)\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Define the input shape for the angle dataset\n",
        "angle_input_shape = (1,)\n",
        "\n",
        "# Create the VGG-16 model\n",
        "model = build_model()\n",
        "\n",
        "# Define the mean squared error as the loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=mse_loss)\n",
        "\n",
        "# Modify the fit call to pass both inputs\n",
        "train_dataset = train_dataset.map(lambda img, angle: ((img, angle), angle))\n",
        "val_dataset = val_dataset.map(lambda img, angle: ((img, angle), angle))"
      ],
      "metadata": {
        "id": "hwcojnuXo4Fx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 50\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMoym7S0qb-T",
        "outputId": "7dad2c5a-29e9-45f0-eb1e-1b6409184f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1136/1136 [==============================] - 670s 580ms/step - loss: 74.3936 - val_loss: 223.4441\n",
            "Epoch 2/50\n",
            "1136/1136 [==============================] - 650s 569ms/step - loss: 1.9753 - val_loss: 105.1200\n",
            "Epoch 3/50\n",
            "1136/1136 [==============================] - 619s 541ms/step - loss: 0.0260 - val_loss: 98.8649\n",
            "Epoch 4/50\n",
            "1136/1136 [==============================] - 619s 541ms/step - loss: 0.1576 - val_loss: 97.2273\n",
            "Epoch 5/50\n",
            "1136/1136 [==============================] - 614s 537ms/step - loss: 0.0151 - val_loss: 131.3139\n",
            "Epoch 6/50\n",
            "1136/1136 [==============================] - 608s 532ms/step - loss: 4.0675 - val_loss: 75.6942\n",
            "Epoch 7/50\n",
            "   3/1136 [..............................] - ETA: 9:12 - loss: 0.0037"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# marker_mapping = {\n",
        "#     0xffd8: \"Start of Image\",\n",
        "#     0xffe0: \"Application Default Header\",\n",
        "#     0xffdb: \"Quantization Table\",\n",
        "#     0xffc0: \"Start of Frame\",\n",
        "#     0xffc4: \"Define Huffman Table\",\n",
        "#     0xffda: \"Start of Scan\",\n",
        "#     0xffd9: \"End of Image\"\n",
        "# }\n",
        "\n",
        "\n",
        "# class JPEG:\n",
        "#     def __init__(self, image_file):\n",
        "#         with open(image_file, 'rb') as f:\n",
        "#             self.img_data = f.read()\n",
        "\n",
        "#     def decode(self):\n",
        "#         data = self.img_data\n",
        "#         while(True):\n",
        "#             marker, = unpack(\">H\", data[0:2])\n",
        "#             # print(marker_mapping.get(marker))\n",
        "#             if marker == 0xffd8:\n",
        "#                 data = data[2:]\n",
        "#             elif marker == 0xffd9:\n",
        "#                 return\n",
        "#             elif marker == 0xffda:\n",
        "#                 data = data[-2:]\n",
        "#             else:\n",
        "#                 lenchunk, = unpack(\">H\", data[2:4])\n",
        "#                 data = data[2+lenchunk:]\n",
        "#             if len(data)==0:\n",
        "#                raise TypeError(\"issue reading jpeg file\")\n",
        "\n",
        "\n",
        "# bads = []\n",
        "\n",
        "# for dirName, subdirList, fileList in os.walk(img_dir):\n",
        "#     imagesList = fileList\n",
        "#     for img in tqdm(imagesList):\n",
        "#       image = os.path.join(img_dir,img)\n",
        "#       image = JPEG(image)\n",
        "#       try:\n",
        "#         image.decode()\n",
        "#       except:\n",
        "#         bads.append(img)\n",
        "\n",
        "\n",
        "# for name in bads:\n",
        "#   os.remove(os.path.join(img_dir,name))"
      ],
      "metadata": {
        "id": "dx_QL2romLiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPJVoAUlrSih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}