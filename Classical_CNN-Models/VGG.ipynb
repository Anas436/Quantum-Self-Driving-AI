{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-06T18:38:18.759974Z","iopub.execute_input":"2023-07-06T18:38:18.760983Z","iopub.status.idle":"2023-07-06T18:38:18.767728Z","shell.execute_reply.started":"2023-07-06T18:38:18.760947Z","shell.execute_reply":"2023-07-06T18:38:18.766885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/raghav-thiruv/Omdena-Quantum-Self-Driving.git","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:27:20.729124Z","iopub.execute_input":"2023-07-07T00:27:20.729513Z","iopub.status.idle":"2023-07-07T00:29:31.493425Z","shell.execute_reply.started":"2023-07-07T00:27:20.729476Z","shell.execute_reply":"2023-07-07T00:29:31.492164Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'Omdena-Quantum-Self-Driving'...\nremote: Enumerating objects: 45761, done.\u001b[K\nremote: Counting objects: 100% (11671/11671), done.\u001b[K\nremote: Compressing objects: 100% (11653/11653), done.\u001b[K\nremote: Total 45761 (delta 32), reused 11645 (delta 17), pack-reused 34090\u001b[K\nReceiving objects: 100% (45761/45761), 2.16 GiB | 21.21 MiB/s, done.\nResolving deltas: 100% (61/61), done.\nUpdating files: 100% (45593/45593), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:30.436329Z","iopub.execute_input":"2023-07-07T00:31:30.436745Z","iopub.status.idle":"2023-07-07T00:31:31.370854Z","shell.execute_reply.started":"2023-07-07T00:31:30.436710Z","shell.execute_reply":"2023-07-07T00:31:31.369649Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Omdena-Quantum-Self-Driving  __notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"!mv \"Omdena-Quantum-Self-Driving/Images/driving_dataset1/data.txt\" \"Omdena-Quantum-Self-Driving/Images/\"","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:32.143788Z","iopub.execute_input":"2023-07-07T00:31:32.144212Z","iopub.status.idle":"2023-07-07T00:31:33.102884Z","shell.execute_reply.started":"2023-07-07T00:31:32.144179Z","shell.execute_reply":"2023-07-07T00:31:33.101415Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from struct import unpack\nfrom tqdm import tqdm\nimport os\n\nimg_dir = 'Omdena-Quantum-Self-Driving/Images/driving_dataset1/'\ndata_file = \"Omdena-Quantum-Self-Driving/Images/data.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:34.337939Z","iopub.execute_input":"2023-07-07T00:31:34.338317Z","iopub.status.idle":"2023-07-07T00:31:34.344072Z","shell.execute_reply.started":"2023-07-07T00:31:34.338283Z","shell.execute_reply":"2023-07-07T00:31:34.343044Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Concatenate, Dropout\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:34.992799Z","iopub.execute_input":"2023-07-07T00:31:34.993539Z","iopub.status.idle":"2023-07-07T00:31:43.265372Z","shell.execute_reply.started":"2023-07-07T00:31:34.993503Z","shell.execute_reply":"2023-07-07T00:31:43.262356Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_model():\n    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n    for layer in base_model.layers:\n        layer.trainable = True\n\n    flatten_output = Flatten()(base_model.output)\n\n    angle_input = Input(shape=(1,))\n    fused_output = Concatenate()([flatten_output, angle_input])\n\n    fc1 = Dense(512, activation='relu')(fused_output)\n    dropout1 = Dropout(0.3)(fc1)\n    fc2 = Dense(256, activation='relu')(fc1)\n    output = Dense(1)(fc2)\n\n    model = Model(inputs=[base_model.input, angle_input], outputs=output)\n\n    return model\n\n\n# Load and preprocess a single image\ndef load_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, (224, 224))\n    image = tf.keras.applications.vgg19.preprocess_input(image)\n    return image\n\ndef create_dataset(image_files, angles):\n    image_paths = [os.path.join(img_dir, file_name) for file_name in image_files]\n    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n    angle_dataset = tf.data.Dataset.from_tensor_slices(angles)\n    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = tf.data.Dataset.zip((image_dataset, angle_dataset))\n    return dataset\n\n# Get the list of image files\nimage_files = []\n\n# Load the steering angles\nangles_dict = {}\nwith open(data_file, 'r') as file:\n    for line in file:\n        img_file, angle = line.strip().split()\n        image_files.append(img_file)\n        angles_dict[img_file] = float(angle)\n\n# Get the angles for the image files\nangles = [angles_dict[file_name] for file_name in image_files]\n\n# Split the image files into training and validation sets\nvalidation_split = 0.2\nnum_validation_samples = int(validation_split * len(image_files))\ntrain_image_files = image_files[num_validation_samples:]\ntrain_image_angles = angles[num_validation_samples:]\nval_image_files = image_files[:num_validation_samples]\nval_image_angles = angles[:num_validation_samples]\n\n# Create the training and validation datasets with both inputs\ntrain_dataset = create_dataset(train_image_files, train_image_angles)\nval_dataset = create_dataset(val_image_files, val_image_angles)\n\n# Shuffle and batch the datasets\nbatch_size = 32\ntrain_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\n\n# Define the input shape for the angle dataset\nangle_input_shape = (1,)\n\n# Create the VGG-16 model\nmodel = build_model()\n\n# Define the mean squared error as the loss function\ndef mse_loss(y_true, y_pred):\n    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=mse_loss)\n\n# Modify the fit call to pass both inputs\ntrain_dataset = train_dataset.map(lambda img, angle: ((img, angle), angle))\nval_dataset = val_dataset.map(lambda img, angle: ((img, angle), angle))","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:43.267937Z","iopub.execute_input":"2023-07-07T00:31:43.269196Z","iopub.status.idle":"2023-07-07T00:31:48.144732Z","shell.execute_reply.started":"2023-07-07T00:31:43.269158Z","shell.execute_reply":"2023-07-07T00:31:48.143807Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nepochs = 50\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:31:48.146112Z","iopub.execute_input":"2023-07-07T00:31:48.146493Z","iopub.status.idle":"2023-07-07T05:06:28.167023Z","shell.execute_reply.started":"2023-07-07T00:31:48.146458Z","shell.execute_reply":"2023-07-07T05:06:28.166018Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/50\n1136/1136 [==============================] - 343s 282ms/step - loss: 71.4111 - val_loss: 359.8236\nEpoch 2/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 1.0737 - val_loss: 291.2111\nEpoch 3/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0380 - val_loss: 290.9698\nEpoch 4/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0313 - val_loss: 310.1879\nEpoch 5/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.8157 - val_loss: 182.8761\nEpoch 6/50\n1136/1136 [==============================] - 314s 273ms/step - loss: 0.1461 - val_loss: 157.3448\nEpoch 7/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0081 - val_loss: 161.6421\nEpoch 8/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 1.0842 - val_loss: 158.7609\nEpoch 9/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.8085 - val_loss: 61.7568\nEpoch 10/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0061 - val_loss: 56.0862\nEpoch 11/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0075 - val_loss: 64.0624\nEpoch 12/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0292 - val_loss: 75.9889\nEpoch 13/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0663 - val_loss: 87.0374\nEpoch 14/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0334 - val_loss: 115.2690\nEpoch 15/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0761 - val_loss: 57.2292\nEpoch 16/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0264 - val_loss: 66.1840\nEpoch 17/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0579 - val_loss: 52.3636\nEpoch 18/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0394 - val_loss: 46.5287\nEpoch 19/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0763 - val_loss: 73.7328\nEpoch 20/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0162 - val_loss: 50.3477\nEpoch 21/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0383 - val_loss: 51.0327\nEpoch 22/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0506 - val_loss: 64.3218\nEpoch 23/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0096 - val_loss: 59.2726\nEpoch 24/50\n1136/1136 [==============================] - 314s 273ms/step - loss: 0.0355 - val_loss: 43.0767\nEpoch 25/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0602 - val_loss: 40.8318\nEpoch 26/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0092 - val_loss: 46.3604\nEpoch 27/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0447 - val_loss: 52.8440\nEpoch 28/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0067 - val_loss: 48.6519\nEpoch 29/50\n1136/1136 [==============================] - 313s 273ms/step - loss: 0.0307 - val_loss: 44.3835\nEpoch 30/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0182 - val_loss: 43.3490\nEpoch 31/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0293 - val_loss: 48.0258\nEpoch 32/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0281 - val_loss: 55.6480\nEpoch 33/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0124 - val_loss: 49.4487\nEpoch 34/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0329 - val_loss: 40.2496\nEpoch 35/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0594 - val_loss: 58.5319\nEpoch 36/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0347 - val_loss: 46.2083\nEpoch 37/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0035 - val_loss: 48.7958\nEpoch 38/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0370 - val_loss: 40.7971\nEpoch 39/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0092 - val_loss: 45.4424\nEpoch 40/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0435 - val_loss: 41.9254\nEpoch 41/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0021 - val_loss: 38.9551\nEpoch 42/50\n1136/1136 [==============================] - 323s 281ms/step - loss: 0.0372 - val_loss: 32.5793\nEpoch 43/50\n1136/1136 [==============================] - 322s 280ms/step - loss: 0.0066 - val_loss: 36.9170\nEpoch 44/50\n1136/1136 [==============================] - 322s 281ms/step - loss: 0.0263 - val_loss: 31.4485\nEpoch 45/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0994 - val_loss: 70.0416\nEpoch 46/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0245 - val_loss: 38.2122\nEpoch 47/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0051 - val_loss: 35.4058\nEpoch 48/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0150 - val_loss: 35.8465\nEpoch 49/50\n1136/1136 [==============================] - 313s 272ms/step - loss: 0.0175 - val_loss: 21.8949\nEpoch 50/50\n1136/1136 [==============================] - 312s 272ms/step - loss: 0.0153 - val_loss: 21.0259\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x797466f4c910>"},"metadata":{}}]},{"cell_type":"code","source":"import random\nfrom tensorflow.keras.preprocessing import image as keras_image\nimport numpy as np\n\ndef load_image(img_dir, target_size=(224, 224)):\n    img = keras_image.load_img(image_path, target_size=target_size)\n    img_tensor = keras_image.img_to_array(img)\n    img_tensor = np.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.keras.applications.resnet.preprocess_input(img_tensor)\n    return img_tensor\n\n# Randomly select an image from validation set\nrandom_image_path = random.choice(val_image_files['filename'].tolist())\n\n# Load the image\ntest_image = load_image(random_image_path)\n\n# Use the model to predict the steering angle for the test image\npredicted_angle = model.predict(test_image)\n\n# Print out the predicted steering angle\nprint(\"Predicted steering angle: \", predicted_angle[0][0])\n\n# If you want to compare this prediction to the actual angle, you could find that as follows:\nactual_angle = val_image_files[val_image_files['filename'] == random_image_path]['steering_angle'].values[0]\nprint(\"Actual steering angle: \", actual_angle)\n","metadata":{},"execution_count":null,"outputs":[]}]}