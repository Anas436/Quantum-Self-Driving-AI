# -*- coding: utf-8 -*-
"""cnn_utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pwaqxf0BJSb1bejtLtwxLJwFWfJZMKkD
"""
import tensorflow as tf
from tf.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import pandas as pd
import streamlit as st
import os


# Load steering angles from text file
df = pd.read_csv('/Users/ansonantony/Desktop/Omdena/Quantum_Self_Driving/Omdena-Quantum-Self-Driving/Images/driving_dataset1/data.txt', names=['filename', 'steering_angle'], delimiter=' ')
image_dir = '/Users/ansonantony/Desktop/Omdena/Quantum_Self_Driving/Omdena-Quantum-Self-Driving/Images/driving_dataset1/'
df['filename'] = df['filename'].apply(lambda x: os.path.join(image_dir, x))

def load_image(image_path):
    try:
        image = tf.io.read_file(image_path)
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, (224, 224))
        image = image / 255.0  # normalize to [0,1] range
    except:
        print(f"Invalid image format, skipping: {image_path}")
        return None
    return image

def create_dataset(df):
    image_dataset = tf.data.Dataset.from_tensor_slices(df['filename'])
    angle_dataset = tf.data.Dataset.from_tensor_slices(df['steering_angle'])
    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)
    image_dataset = image_dataset.apply(tf.data.experimental.ignore_errors())
    dataset = tf.data.Dataset.zip((image_dataset, angle_dataset))

    return dataset

validation_split = 0.1
df = df.sample(frac=1).reset_index(drop=True)
val_df = df[:int(validation_split*len(df))]
train_df = df[int(validation_split*len(df)):]

train_dataset = create_dataset(train_df)
val_dataset = create_dataset(val_df)

batch_size = 32
train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)
val_dataset = val_dataset.batch(batch_size)

def build_model(layers):
    model = Sequential()
    for i, layer in enumerate(layers):
        layer_type, params = layer[0], layer[1:]
        if layer_type == 'CNN':
            if i == 0:
                # specify input_shape for first layer
                model.add(Conv2D(filters=32, kernel_size=params[0], padding='same', activation='relu', input_shape=(64, 64, 3)))
            else:
                model.add(Conv2D(filters=32, kernel_size=params[0], padding='same', activation='relu'))
        # elif layer_type == 'Activation':
        #     model.add(Activation('relu'))
        elif layer_type == 'MaxPool':
            model.add(MaxPooling2D(pool_size=(2, 2)))
        elif layer_type == 'Dense':
            model.add(Dense(units=params[0], activation='relu'))
        elif layer_type == 'Flatten':
            model.add(Flatten())

    # Add final layer with 1 output unit
    model.add(Dense(units=1))

    return model

def train_cnn():
    st.title("Train Your CNN Network")
    # Add content for the train CNN page
    st.title("Train Your CNN Network")

    if 'model' not in st.session_state:
        st.warning("Please create a CNN first.")
        return

    model = st.session_state['model']

    # specify some training parameters
    epochs = st.number_input('Enter number of epochs', min_value=1, max_value=100, value=10)
    batch_size = st.number_input('Enter batch size', min_value=1, max_value=1000, value=32)

    model.compile(optimizer='adam', loss='mean_squared_error')  # assuming a regression problem

    # initiate early stopping
    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
    mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

    # Load the data as done in the original script
    validation_split = 0.1
    df = df.sample(frac=1).reset_index(drop=True)
    val_df = df[:int(validation_split*len(df))]
    train_df = df[int(validation_split*len(df)):]

    train_dataset = create_dataset(train_df)
    val_dataset = create_dataset(val_df)

    train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)
    val_dataset = val_dataset.batch(batch_size)

    # fit the model
    history = model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[es, mc], workers=4)

    # save history into session state
    st.session_state['history'] = history.history

    # Show success message
    st.success("Training is completed!")

